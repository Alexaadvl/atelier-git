{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexaadvl/atelier-git/blob/main/LLMs_translate_TP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Use Case: Translate Sentence\n",
        "\n",
        "Welcome to the practical side of LLMs part 2.\n",
        "\n",
        "In this course you will do the translation task using generative AI. You will explore how the input text affects the output of the model, and perform prompt engineering to direct it towards the task you need. By comparing zero shot, one shot, and few shot inferences, you will take the first step towards prompt engineering and see how it can enhance the generative output of Large Language Models.\n",
        "\n",
        "At the end you will try to score your model."
      ],
      "metadata": {
        "id": "aFRgrYcT3b8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0 - installation\n"
      ],
      "metadata": {
        "id": "2S2mksko3-Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required packages to use PyTorch and Hugging Face transformers and datasets.\n"
      ],
      "metadata": {
        "id": "_PE7WkSK36xE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVEId2qs3Z4q"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install -q transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Load dataset and play !"
      ],
      "metadata": {
        "id": "V3ug2m2P3bDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : Let's upload some simple translations dataset from  Hugging Face dataset. Make sure your dataset is containing en - fr data\n",
        "\n",
        "https://huggingface.co/datasets?task_categories=task_categories:translation&sort=trending"
      ],
      "metadata": {
        "id": "sjurAEAD42xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "sentences ="
      ],
      "metadata": {
        "id": "IArioI3v4rYz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : Print a couple of dialogues with their baseline summaries."
      ],
      "metadata": {
        "id": "4pG66jzm5KV_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnYVqtt65Exd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CawMICQfC6uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : Load Flan-T5 model and it's associate tokenizer\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/flan-t5"
      ],
      "metadata": {
        "id": "dO9kEfp_5erj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uH87AwE35Oef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : Download the tokenizer for the FLAN-T5 model\n",
        "\n",
        "https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer"
      ],
      "metadata": {
        "id": "dy1txQmN52LP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "roJF1Gb35s6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPqw_KX95_Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : Test the tokenizer encoding and decoding a simple sentence:"
      ],
      "metadata": {
        "id": "uezKnqH96JLj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBg79ILe6C3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOsXKbqpEAhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Translate sentence without an Instruction Prompt"
      ],
      "metadata": {
        "id": "WX4Ddmpa6koG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : Now it's time to explore how well the base LLM translate a sentence without any prompt engineering"
      ],
      "metadata": {
        "id": "Dnx9y8PZ6uCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1/ je recupere un texte\n",
        "\n",
        "\n",
        "# 2 je le tokenize\n",
        "\n",
        "\n",
        "# 3 model\n",
        "\n",
        "\n",
        "# 4 detokenize l'output"
      ],
      "metadata": {
        "id": "cbufjY846QkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_en)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "print(translation_m)"
      ],
      "metadata": {
        "id": "2sUX9jXPFkW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is your model performing well ???"
      ],
      "metadata": {
        "id": "Xfta6S1c69dK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NO"
      ],
      "metadata": {
        "id": "SXQKvjI2WZiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Translate sentence with an Instruction Prompt"
      ],
      "metadata": {
        "id": "vlMmZuUk7CaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1 - Zero Shot Inference with an Instruction Prompt"
      ],
      "metadata": {
        "id": "dHAA3Xup7OHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to instruct the model to perform a task - translate the sentence - you can take the en sentence and convert it into an instruction prompt. This is often called zero shot inference.\n"
      ],
      "metadata": {
        "id": "eHy6r2yE7ddH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Wrap the dialogue in a descriptive instruction and see how the generated text will change:"
      ],
      "metadata": {
        "id": "6h8O-rJO7jjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DB7Sm6W461eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 je le tokenize\n",
        "\n",
        "\n",
        "# 3 model\n",
        "\n",
        "# 4 detokenize l'output"
      ],
      "metadata": {
        "id": "wg65SU11HRuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_en)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "print(translation_m)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "print(translation_m_zero_shot)"
      ],
      "metadata": {
        "id": "8a2HdoWmHmoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is your task performing better ?"
      ],
      "metadata": {
        "id": "K1ZmT3wK79TD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going further : Experiment with the prompt text and see how the inferences will be changed. Will the inferences change if you end the prompt with just empty string vs. translate: ?"
      ],
      "metadata": {
        "id": "kaG8GfNS8QTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5"
      ],
      "metadata": {
        "id": "E3hC8Q058ASN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Let's use a slightly different prompt. FLAN-T5 has many prompt templates that are published for certain tasks here. In the following code, you will use one of the pre-built FLAN-T5 prompts\n",
        "\n",
        "https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py"
      ],
      "metadata": {
        "id": "1Pl5XcGz8a05"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qADeU3Eg8aJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is your conclusion ?"
      ],
      "metadata": {
        "id": "AtB8OQQk8zA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Translate Dialogue with One Shot and Few Shot Inference"
      ],
      "metadata": {
        "id": "-i8EZ9V285vO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One shot and few shot inference are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task. You can read more about it in this blog from HuggingFace :    \n",
        "\n",
        "\n",
        "https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api"
      ],
      "metadata": {
        "id": "HHSrHC6d8-oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.1 - One Shot Inference"
      ],
      "metadata": {
        "id": "pI2JD63h9P0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : Build a function that takes a list of sentence (en and fr), generates a prompt with full examples, then at the end appends the prompt which you want the model to translate. You will use the same FLAN-T5 prompt template from section 3.2.\n",
        "\n",
        "Build your own function !!!!"
      ],
      "metadata": {
        "id": "8KT5lzCK9ZPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "\n",
        "   return"
      ],
      "metadata": {
        "id": "ayIi3RRd8ueL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : Construct the prompt to perform one shot inference:"
      ],
      "metadata": {
        "id": "FCNVvDaB905c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t0iHM08k9sJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gi8PPi3vK_My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Pass this prompt to perform the one shot inference:"
      ],
      "metadata": {
        "id": "Sqh3noyi96tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 je le tokenize\n",
        "\n",
        "# 3 model\n",
        "\n",
        "# 4 detokenize l'output"
      ],
      "metadata": {
        "id": "yfB6jelh95on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is your model improving ? why ?"
      ],
      "metadata": {
        "id": "kAUj1o9u-Ks_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 - Few Shot Inference"
      ],
      "metadata": {
        "id": "sMvsjQjG-Ros"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Let's explore few shot inference by adding two more full dialogue-summary pairs to your prompt."
      ],
      "metadata": {
        "id": "odeRLmbg-WtL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfKZPp-b-BCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Now pass this prompt to perform a few shot inference"
      ],
      "metadata": {
        "id": "hKtoYyDs-in9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 je le tokenize\n",
        "\n",
        "# 3 model\n",
        "\n",
        "# 4 detokenize l'output"
      ],
      "metadata": {
        "id": "SZCW33Oi-aC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "Experiment with the few shot inferencing.\n",
        "\n",
        "Choose different dialogues - change the indices in the example_indices_full list and example_index_to_summarize value.\n",
        "Change the number of shots. Be sure to stay within the model's 512 context length, however.\n",
        "How well does few shot inferencing work with other examples?"
      ],
      "metadata": {
        "id": "qzhttKs4-wgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - Generative Configuration Parameters for Inference"
      ],
      "metadata": {
        "id": "wAkGZw-2-4xH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can change the configuration parameters of the generate() method to see a different output from the LLM. So far the only parameter that you have been setting was max_new_tokens=50, which defines the maximum number of tokens to generate. A full list of available parameters can be found in the Hugging Face Generation documentation :    \n",
        "\n",
        "\n",
        "https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig"
      ],
      "metadata": {
        "id": "Pbitb95x_AAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Change the configuration parameters to investigate their influence on the output."
      ],
      "metadata": {
        "id": "hoAPn-SX_JvC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BTYpZuAn_U5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tGzd6Yov-p_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the effect of reducing the param : max_new_tokens ?\n",
        "\n",
        "\n",
        "What is the impact of Putting do_sample ?"
      ],
      "metadata": {
        "id": "FgKAe6-M_hqk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXoJkQva_wt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 - Model evaluation\n"
      ],
      "metadata": {
        "id": "7CZVCkx9EtRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install evaluate==0.4.0"
      ],
      "metadata": {
        "id": "ZnQl1sGaExer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate"
      ],
      "metadata": {
        "id": "euvFFqg4E1hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.\n",
        "\n",
        "TODO: create the ROUGE evaluator\n",
        "\n",
        "doc :\n",
        "https://huggingface.co/spaces/evaluate-metric/rouge"
      ],
      "metadata": {
        "id": "ZujNxwKbE-Um"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFWErk-2E8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Evaluate the models computing ROUGE metrics for 20% of sentences.\n"
      ],
      "metadata": {
        "id": "zFV-GJ7LFW9L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3AIQAHmkFXfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : Use another evaluation (BLEU or SACREBLEU)"
      ],
      "metadata": {
        "id": "ZqgflWTjfXrH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kPMT1vcVfc3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}